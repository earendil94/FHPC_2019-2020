{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Assigment  FHPC course \n",
    "\n",
    "\n",
    "### Francesco Brand\n",
    "\n",
    "###  07/11/2019\n",
    "\n",
    "### Academic year 2019/2020\n",
    "\n",
    "\n",
    "## Section 0: (warm-up)\n",
    "\n",
    "\n",
    "In this first section we are going to compare the theoretical peak performance of our laptop and the actual sustained performance of our smartphone to the top 500 rank of supercomputers over the years.\n",
    " \n",
    "### Laptop \n",
    "\n",
    "For what regards the specific of my laptop, the CPU, Frequency and number of cores are easily obtained by looking at the intel webpage of my processor. In the same webpage, we can obtain the family of products to which our processor belong (in our case \"Whiskey Lake\") and on the wikipedia page suggested in the assignment we can see that the Whiskey Lake family processors share the same low-level architecture, which is capable of producing 16FLOP/cycle. Therefore this is the sustained peak performance, considering the regular clock frequency and the boosted frequency.\n",
    "  \n",
    "\n",
    "  |          Model           |     CPU      | Frequency | number  of core | Peak performance |\n",
    "  |  -----------------------------| ------------ | --------- | --------------- | ---------------- |\n",
    "  | HP Pavilion Laptop 14-ce2xxx |   i5-8265U   |  1,60 GHz |        4        |   102,4 GFLOPs   |\n",
    "  | HP Pavilion Laptop 14-ce2xxx |   i5-8265U   |  3,90 GHz |        4        |   249,6 GFLOPs   |\n",
    "                                                             \n",
    " \n",
    "### Smartphone\n",
    "\n",
    "  \n",
    "For the smartphone, we just ran the linpack benchmark test provided by the assignment at \n",
    "https://play.google.com/store/apps/details?id=com.sqi.linpackbenchmark and we got these results\n",
    "\n",
    "  |  Your model | sustained performance | size of matrix | \n",
    "  |  ---------- | --------------------- | -------------- | \n",
    "  |     Nokia TA-1032       |       330 MFLOPs        |         2500       | \n",
    "\n",
    "\n",
    "\n",
    "According to top500.org, my smartphone would not make the cut into the top 500 neither on its debut year. However this is the placement of my laptop, considering its theoretical peak performance for the normal frequency of usage. In the table below NPE means \"not performant enough\".\n",
    "\n",
    "\n",
    "  |  Your model | performance | Top500 year | number 1 HPC system | number of processors (TOP500) |\n",
    "  | ---------- | ----------- | ----------- | ------------------- | ----------------------------- |\n",
    "  |   Nokia TA-1032   |   330 MFLOPs   |     NPE     |     NPE   |                               |\n",
    "  |   HP Pavilion Laptop 14-ce2xxx  |  102,4 GFLOPs  | Rank 3, Dec 95 | Rank 1, June 93 |  140                           |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section  1: theoretical model\n",
    "\n",
    "_I've decided to leave the premises for this section and to answer directly to the questions below them, in order to have a clearer layout._\n",
    "\n",
    "- devise a performance model for a simple parallel algorithm: sum of N numbers\n",
    "\n",
    "  - Serial Algorithm : n-1 operations \n",
    "\n",
    "    $T_{serial}= N*T_{comp}$ \n",
    "    \n",
    "    where $T_{comp}$= *time to compute a floating point operation*\n",
    "\n",
    "  - Parallel Algorithm : master-slave\t\n",
    "\n",
    "    We decided not to read from an input file the number to compute but to pass it to the program as a command line argument.\n",
    "    \n",
    "    - Distribute N to P-1  slaves ===>  $(P-1)\\times T_{comm}$ \n",
    "    \n",
    "      where $T_{comm}$ = *time  each processor takes to communicate one message, i.e. latency..*\n",
    "\n",
    "    - N/P sum over each processors (including master)  ===> $(N/P + 2)  T_{comp}$ \n",
    "    \n",
    "      (The +2 here is explained in more detail in section 3)\n",
    "\n",
    "    - Slaves send partial sum  ===>   $(P-1)T_{comm}$\n",
    "\n",
    "    - Master performs  one final sum ===>  $(P-1) \\times T_{comp}$\n",
    "\n",
    "      the final model: $T_{p} =  T_{comp} \\times (P + 1 + n/P) + 2(P-1) \\times T_{comm}$\n",
    "      \n",
    "      \n",
    "      - compute scalability curves for such algorithm and make some plots\n",
    "\n",
    "  - assumptions:\n",
    "\n",
    "    - $T_{comp} =2 \\times 10^{-9}$\n",
    "    - $T_{read}= 1 \\times 10^{-4}$\n",
    "    - $T_{comm}= 1 \\times 10^{-6}$\n",
    "\n",
    "    Play with some value of N and plot against P  (with P ranging from 1 to 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  ![TheoreticalPartialSumSpeedup](img/Theoretical_speedup_partial_sum.png)\n",
    "\n",
    "- Comment on them:\n",
    "\n",
    "  **- For which values of N do you see the algorithm scaling ?**\n",
    "      \n",
    "   The algorithm doesn't really scale at all if we look to all the plots below $N = 10^{10}$. The speedud sharply rises up until a certain $P_{max} $ only to then decrease to 0 for high values of P. Only for $N=10^{11}$ we can say that our algorithm should strongly scale on the quantity of processors considered here.\n",
    "   This behaviour is especially caused by the $T_{comm}$ factor which, scaling linearly with respect to P, doesn't make our model strongly scale for any fixed value of N, from a theoretical perspective.\n",
    "\n",
    "    **- For which values of P does the algorithm produce the best results ?**\n",
    "  \n",
    "    These are the P values yielding the best speedup results for the different values of N:    \n",
    " \n",
    " \n",
    " |      n      |     $P_{max}$    |   \n",
    " | ----------- | --------------- | \n",
    " |   $10^3$    |       1         | \n",
    " |   $10^4$    |       3         | \n",
    " |   $10^5$    |       8         | \n",
    " |   $10^6$    |       26        | \n",
    " |   $10^7$    |       82        | \n",
    " |   $10^8$    |       258       | \n",
    " |   $10^9$    |       816       | \n",
    " |   $10^{10}$ |      >2000      | \n",
    " |   $10^{11}$ |      >2000      |    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "  **- Can you try to modify the algorithm sketched above to increase its scalability ? **\n",
    "  \n",
    "  Yes! We can for example redesign both the communication from the master to the slaves and viceversa in this way:\n",
    "  \n",
    "  - Instead of communicating N to P-1 slaves, we could send the information just to $\\sqrt{P-1}$ slaves and then make each one of them pass the same information (i.e. N) to $\\sqrt{P-1} - 1$ slaves. This way our initial communication term will be equal to = $2 \\times \\sqrt{P-1} - 1$ .\n",
    "  \n",
    "  - The same thing can be done for the inverse path. Consider a collection of different $\\sqrt{P-1} - 1$ slaves and make them all send their partial sum to their respective \"sub-master\" simultaneously. Every $\\sqrt{P-1}$ sub-master computes the partial sum and then send the result to the master repeating this very same process for each submaster.\n",
    "  \n",
    "  This way our total communication term will be reduced to $(4 \\times \\sqrt{P-1} - 2) \\times T_{comm}$ and our computational term will be $(2 \\times \\sqrt{P-1} -2 + n/P) \\times T_{comp}$.\n",
    "  \n",
    "  \n",
    " Hence, the final corrected model would be: \n",
    " $ T_{p}= T_{comp} \\times ( 2  \\sqrt{P-1} -2 + n/P)  + (4 \\sqrt{P-1} - 2) \\times T_{comm}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : play with MPI program\n",
    "\n",
    "### 2.1:  compute strong scalability of a mpi_pi.c program\n",
    "\n",
    "In this section we will briefly describe the results of the analysis of the different execution time yielded by the code provided in class to compute PI using Monte-Carlo integration. The results refer both to the execution of the serial algorithm and to the parallel one.\n",
    "\n",
    "#### Time measurement considerations\n",
    "\n",
    "Firstly, it needs to be addressed the problem regarding the measure of time of an execution of a code. To measure the execution time of the serial algorithm, the idea is to use the command /usr/bin/time, as suggested by the assignment. This standard shell command yields three different measures of time: elapsed time, user time and system time. Since we are interested in the actual runtime of our application, out of these three quantities we will just consider the elapsed time. Time measures will also be reported in milliseconds: we decided to go for this time scale since some of the time differences got to small for the data elaboration to work properly, hence we decided to scale directly the input data.\n",
    "\n",
    "\n",
    "  |              |  n=$10^6$ |  n=$10^7$ |  n=$10^8$ |  n=$10^9$ |   \n",
    "  | ----------   | ----------  | ---------   |  ---------- |  ----------  |  \n",
    "  | Elapsed time (ms) |    10.000   |    190.000  |    1990.000   |   19730.000    | \n",
    "  \n",
    "_Elapsed time captured for the execution of the serial algorithm. We can see that as n increases of a factor 10 for every entry of the table, so does the elapsed time_\n",
    " \n",
    "\n",
    "To identify the overhead of MPI routines, we can just consider the difference in between the elapsed time returned by the serial program, and the one returned by the parallel program using just one processor: since the computational part is identical in this case, this difference is a good estimation of the time needed to call and execute the MPI functions included in the PI code. Moreover, we expect the difference to be constant for different values of N, since the computational part of the two executions is the same. \n",
    "\n",
    "\n",
    "\n",
    "  |                             |  n=$10^6$ |  n=$10^7$ |  n=$10^8$ |  n=$10^9$ |   \n",
    "  | ----------                  | ----------  | ---------   |  ---------- |  ----------  |  \n",
    "  | E_time Serial (ms)          |    10.000   |    190.000  |    1990.000   |   19730.000    | \n",
    "  | E_time Parallel, P = 1 (ms) |  1590.000 | 1700.000 | 3490.000| 21350.000 |\n",
    "  | E_time difference (ms) |  1580.000 | 1510.000 | 1500.000 | 1620.000|\n",
    "  \n",
    "<br>\n",
    "<center><i>Elapsed time captured both for the serial algorithm and for the parallel algorithm with one processor only. The difference between them remains pretty much constant with respect to n. On average, we have that $T_{MPI} = 1552.5 ms$</i></center>\n",
    "\n",
    "\n",
    "\n",
    "In the next section, we will take into consideration both the elapsed time for our parallel code run with multiple processors, and the walltime returned by the program itself. Of all the printed walltimes, our choice will be to take the maximum one, since this will be the measure of time of the last processor to end. Since the program ends when all the processors end their task, this is also a good measure of the lenght of our program. It is worth noting that this processor is not necessarily going to be the master due to the effect of various synchronization overheads affecting our code. More on this on paragraph 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Strong scaling\n",
    "\n",
    "In order to verify the strong scaling of our code, we can plot the different speedup values of our code against the number of processors used. \n",
    "\n",
    "<br>\n",
    "<center>$Speedup(P) := T(1)/T(P)$</center>\n",
    "\n",
    "We will acquire the data in the way that we've described in the previous paragraph, both considering the maximum walltime out of all the processors and the elapsed time of the program execution. Moreover we will take the elapsed time of execution of the serial code as a good measure of T(1). We will also plot these data for 4 different values of N, increasing N by a factor of 10 each time; in this way it will be clear which is the size of N for which we can say that our code scales strongly.\n",
    "\n",
    "![](img/StrongScalingMontecarloPi.png)\n",
    "\n",
    "The image shows for different values of N the value of the execution time for increasing P. Focusing just on the walltime curves, as can be seen from the plots, for every N our walltime curve has a tendency to increase with respect to P, and this tendency becomes increasingly linear the higher the N value. This shows that for high values of N our code will actually scale in the range of P considered, but if we would extend P over a certain limit, no value of N would reasonably make our code scale. This already can be seen for $N=10^6$, where the line never reaches a linear shape. \n",
    "\n",
    "For what regards the elapsed time curve, we can clearly see how lower it is with respect to the walltime one. This is due to the fact that the walltime returned by the program doesn't take into consideration all the system calls, the calls to the initial and final mpi routines, the time spent printing messages on the standard output. All these things are instead considered by the elapsed time, which is simply a measure of the interval of time going from the beginning to the end of the program. Since we've already seen that $T_{MPI} = 1552.5 ms$, the speedup of our parallel code will actually be < 1 (i.e. the serial program is faster than the parallel one) if $T_{MPI}$ is significant in the total time of execution of the program, i.e. for every value of N below $10^8$, and it will never reach a point of strong scaling within the considered values of N. Since the elapsed time of execution of our code has the same trend of the walltime, we've decided to drop the former measure for the next paragraphs since we will need to increase N too much in order to have meaningful results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: identify a model for the parallel overhead\n",
    "\n",
    "As discussed in class, we propose here to calculate the parallel overhead, i.e. the time spent by the parallel part of our code to execute processes not directly related to computing numbers, as the difference in between the maximum and minimum walltime gathered by the execution of the strong scaling test script. Based on what we discussed in class, this time difference should be a good estimate of the parallel overhead of our mpi program since it accesses the time the fastest processor has to wait for the program to finish (which is accounted as a parallel overhead of our code). Since we already know that our master node needs to communicate at least once with all the slave processors, we would expect our model to show the latency time associated to this communication, something which we think it's directly proportional to P-1 (since the master node needs to receive a message from all the other nodes, which are P-1).\n",
    "\n",
    "\n",
    "![ParallelOverhead](img/ParallelOverhead.png)\n",
    "<center><i>$Max (walltime) - min (walltime)$ against P<i></center>\n",
    "\n",
    "What we get is something pretty different though. Except for the plot with $N=10^6$, no plot shows a consistent increasing behaviour with respect to P. The fluctuation in between the data is actually relevant for $N = 10^8$ and $N = 10^9$ which makes it pretty much impossible to understand any type of trend in our data. The model addressed does not seem correct, because it fails to take into consideration at least the latency time we would expect to see.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Weak scaling \n",
    "\n",
    "Doing a weak scaling test means to consider the execution time of an algorithm over a varying number of processor so that $N/P$ remains constant. Therefore, we've decided to choose $N_{0}=10^7, 10^8, 10^9$ as the starting values for our test and then increase N proportionally with the number of P. What we would expect for a perfectly weak scaling code is a constant line with respect to the P axes, because our code is computing exactly $N/P$ operations which is the number that is being kept constant. In practice, as it was for the strong scaling consideration, this does not take into consideration a latency effect in communication which is actually affecting our code. Since the number of communication is increasing with respect to P, our code will tend to increase its runtime by increasing the number of processors. Again, from a theoretical point of view, also the measure of the given efficiency $T(1)/T(P)$ should be increasing as P since our serial code execution time (T(1)) is directly proportional to N (which increases with the same rate of P, by definition of this test) and our denominator T(P) is in theory constant, but this might not be the case for higher values of P for everything we've just described.\n",
    "\n",
    "\n",
    "Here are the results in terms of T(P) and Efficiency(P). The efficiency plot is on the left while the execution time plot is on the right.\n",
    "\n",
    "\n",
    "![WeakScalingPi](img/WeakScalingMontePi.png)\n",
    "\n",
    "\n",
    "\n",
    "The tendencies of the various T(P) are close to what we actually described previously in the paragraph. The efficiency it's increasing though, something that is closer to our actual textbook theory than to our predictions. However, it is still reasonable that this function increases: the factor for which T(1) increases is very close to the multiplying factor of N (i.e. P) but the denominator T(P) grows at a slower rate, as can be deduced from the plots.\n",
    "\n",
    "We can also notice from these data how the random variation of execution time increases for higher values of $N/P$. This is probably due to the fact that the higher the parallelisation gets, the more we are increasing some hidden parallel overheads, such as synchronization times in between the running of the different processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: implement a parallel program using MPI \n",
    "\n",
    "We chose to write the program in C, since it's the language we've been using the most during these first hpc lectures. The code implements an algorithm that follows the model discussed during the lectures. In particular, it performs the following steps:\n",
    "\n",
    "* Defines a master node that reads from command-line argument the number N of which we want to calculate the partial sum.\n",
    "\n",
    "* Sends to P-1 slaves processor the number N/P. In particular, we have decided to communicate to every slave processor the number N, and make them perform two more calculation: n/p and n\\%p. We have done this to solve the problem of redestribution of the remainder of N/P. In fact, calculating the remainder inside the master node and then splitting it to every slave node would have required another communication, thus increasing the actual performance of another $P \\times T_comm$, which is 10^3 times larger than $P \\times T_comp$. After calculating n\\%p, every slave calculates the sum of n/p (+1 if the actual rank of the processor is smaller than n\\%p) numbers in between 0 and N.\n",
    "\n",
    "* Computes all the partial sum computation in every processor, master included\n",
    "\n",
    "* Sends the partial results to the master which receives them and sums together the last P numbers\n",
    "\n",
    "As described in point 2, the algorithm performs two operations per processor more than what we defined in section 1, this is why we have updated the theoretical code in the first section. We are not going to include the extra calculation some of the slave processors need to do because of the remainder of n/p, since it will complicate the model too much without any sensible difference.\n",
    "\n",
    "The code runs perfectly up until $N \\approx 10^{10} $ where the number to be calculated gets too high and the program overflows, despite having defined the largest possible datatype in the source code.\n",
    "\n",
    "Finally, we wrote a second version of the code using the collective MPI operations such as MPI_Broadcast and MPI_Reduce, instead of having to specify both Send and Receive for every communication.\n",
    "\n",
    "More technical information regarding the usage of the code will be given in the readme file provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4  (run and compare)\n",
    "\n",
    "In this section we present the result of the execution of the program built in section 3. Just like in section 2, we present plots regarding a strong scalability test on the algorithm and we comment on them.\n",
    "\n",
    "### Strong Scaling\n",
    "\n",
    "We'll first start by investigating if our implementation of the algorithm presents a strong scalability by making the same plots produced for the Montecarlo Pi algorithm. We'll both plot the strong scalability calculated by taking the maximum walltime returned by our partial sum algorithm and the one obtained considering $T_{read}, T_{comp}$ and $T_{comm}$.\n",
    "\n",
    "![StrongScalingSum](img/SumStrongScaling.png)\n",
    "\n",
    "We can group the performance of these plots in two categories. \n",
    "\n",
    "- The plots for $N=10^6$ and $N=10^7$ show a poor tendency to strong scalability. Both speedups are decreasing for early values of P, respectively for $P \\approx 8$ and $P \\approx 14$, but they are slowing down already from $P \\approx 2$ and $P \\approx 6$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The plots for $N=10^8$ and $N=10^9$ return a weird result. Both are consistently over the perfect speedup S(P) = P; the plot for $N=10^9$ even shows a superlinear tendency (i.e. $bP$ with $b > 1$). This is totally unexpected since there should not be any low level behaviour distinguishing our serial and parallel code. First of all, since we are dealing with a large N (in fact, the previous two plots don't show this behaviour!), it is reasonable to think that this superlinearity is achieved just in the computational part of our code, since this is the most dominant factor of our time estimation, given that the serial part of our parallel program does not scale with N. \n",
    "Then, two possible options are:\n",
    "    - A difference in the code implementation in between the serial and the parallel algorithm (which, if it is the case, we were not able to spot within the time given unfortunately).\n",
    "    - A difference in the level of memory hierarchy used, which though we might exclude based on the fact that the magnitude of our N/P doesn't vary too much in this test, and given the fact that both code have been compiled on the same node of ulysses. So, unless mpicc and gcc are optimizing the algorithm in two different ways, we can exclude this hypothesis.\n",
    "\n",
    "Nevertheless, we can see how the latency time of MPI communication is starting to affect the $N=10^8$ plot by the tail of the curve, since it is starting to slow down, but the same cannot be said for the $N=10^9$ curve.\n",
    "\n",
    "![PracticalVSTheoreticalSU](img/Practical_Theoretical_sum_speedup.png)\n",
    "Also plotting the actual speedup against our model defined in section one, we can see poor results. For every plot except the last one, we can see how the plot are starting to slow down before the expected value of P. This can be a signal that our model is not taking into consideration some further parallel overhead dependent on P, such as for example synchronization time in between our processors. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
